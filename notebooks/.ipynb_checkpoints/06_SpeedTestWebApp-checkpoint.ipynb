{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test deployed web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pulls some images and tests them against the deployed web application. We submit requests asychronously \n",
    "which should reduce the contribution of latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "from azure_utils.configuration.project_configuration import ProjectConfiguration\n",
    "from azure_utils.machine_learning.utils import get_workspace_from_config\n",
    "from azureml.core.webservice import AksWebservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws = get_workspace_from_config()\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_configuration = ProjectConfiguration(\"project.yml\")\n",
    "aks_service_name = project_configuration.get_settings('aks_service_name')\n",
    "aks_service = AksWebservice(ws, name=aks_service_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test our service concurrently but only have 4 concurrent requests at any time. We have only deployed one pod \n",
    "on one node and increasing the number of concurrent calls does not really increase throughput. Feel free to try \n",
    "different values and see how the service responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "concurrent_requests = 4  # Number of requests at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring URL and API key of the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = aks_service.scoring_uri\n",
    "api_key = aks_service.get_keys()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to use [Locust](https://locust.io/) to load test our deployed model. First we need to write the \n",
    "locustfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile locustfile.py\n",
    "from locust import HttpLocust, TaskSet, task\n",
    "import os\n",
    "import pandas as pd\n",
    "from utilities import text_to_json\n",
    "from itertools import cycle\n",
    "\n",
    "_NUMBER_OF_REQUESTS = os.getenv('NUMBER_OF_REQUESTS', 100)\n",
    "dupes_test_path = './data_folder/dupes_test.tsv'\n",
    "dupes_test = pd.read_csv(dupes_test_path, sep='\\t', encoding='latin1')\n",
    "dupes_to_score = dupes_test.iloc[:_NUMBER_OF_REQUESTS, 4]\n",
    "_SCORE_PATH = os.getenv('SCORE_PATH', \"/score\")\n",
    "_API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "\n",
    "class UserBehavior(TaskSet):\n",
    "    def on_start(self):\n",
    "        print('Running setup')\n",
    "        self._text_generator = cycle(dupes_to_score.apply(text_to_json))\n",
    "        self._headers = {\n",
    "            \"content-type\": \"application/json\",\n",
    "            'Authorization': ('Bearer {}'.format(_API_KEY))\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def score(self):\n",
    "        self.client.post(_SCORE_PATH,\n",
    "                         data=next(self._text_generator),\n",
    "                         headers=self._headers)\n",
    "\n",
    "\n",
    "class WebsiteUser(HttpLocust):\n",
    "    task_set = UserBehavior\n",
    "    # min and max time to wait before repeating task\n",
    "    min_wait = 10\n",
    "    max_wait = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the locust command we want to run. We are going to run at a hatch rate of 10 and the whole test will \n",
    "last 1 minute. Feel free to adjust the parameters below and see how the results differ. The results of the test will \n",
    "be saved to two csv files **modeltest_requests.csv** and **modeltest_distribution.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_url = urlparse(scoring_url)\n",
    "cmd = \"locust -H {host} --no-web -c {users} -r {rate} -t {duration} --csv=modeltest --only-summary\".format(\n",
    "    host=\"{url.scheme}://{url.netloc}\".format(url=parsed_url),\n",
    "    users=concurrent_requests,  # concurrent users\n",
    "    rate=10,  # hatch rate (users / second)\n",
    "    duration='1m',  # test duration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! API_KEY={api_key} SCORE_PATH={parsed_url.path} PYTHONPATH={os.path.abspath('../')} {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the summary results of our test and below that the distribution infromation of those tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"modeltest_requests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"modeltest_distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tear down the cluster and all related resources go to the [tear down the cluster](07_TearDown.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "az-ml-realtime-score",
   "language": "python",
   "name": "conda-env-az-ml-realtime-score-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
